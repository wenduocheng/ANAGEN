True
device: cuda
{'weight': 'unet', 'one_hot': False, 'lr': 1e-05, 'batch_size': 256, 'epochs': 100, 'channels': [16, 32, 64], 'drop_out': 0.2}
Encoder_v3(
  (model): Sequential(
    (0): Conv1d(768, 16, kernel_size=(31,), stride=(1,), padding=(15,))
    (1): ResnetBlock(
      (norm1): GroupNorm(16, 16, eps=1e-06, affine=True)
      (conv1): Conv1d(16, 32, kernel_size=(9,), stride=(1,), padding=(4,))
      (norm2): GroupNorm(16, 32, eps=1e-06, affine=True)
      (dropout): Dropout(p=0.2, inplace=False)
      (conv2): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,))
      (nin_shortcut): Conv1d(16, 32, kernel_size=(1,), stride=(1,))
    )
    (2): ResnetBlock(
      (norm1): GroupNorm(16, 32, eps=1e-06, affine=True)
      (conv1): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,))
      (norm2): GroupNorm(16, 32, eps=1e-06, affine=True)
      (dropout): Dropout(p=0.2, inplace=False)
      (conv2): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,))
    )
    (3): ResnetBlock(
      (norm1): GroupNorm(16, 32, eps=1e-06, affine=True)
      (conv1): Conv1d(32, 64, kernel_size=(3,), stride=(1,), padding=(1,))
      (norm2): GroupNorm(16, 64, eps=1e-06, affine=True)
      (dropout): Dropout(p=0.2, inplace=False)
      (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))
      (nin_shortcut): Conv1d(32, 64, kernel_size=(1,), stride=(1,))
    )
    (4): ResnetBlock(
      (norm1): GroupNorm(16, 64, eps=1e-06, affine=True)
      (conv1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))
      (norm2): GroupNorm(16, 64, eps=1e-06, affine=True)
      (dropout): Dropout(p=0.2, inplace=False)
      (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))
    )
    (5): ResnetBlock(
      (norm1): GroupNorm(16, 64, eps=1e-06, affine=True)
      (conv1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))
      (norm2): GroupNorm(16, 64, eps=1e-06, affine=True)
      (dropout): Dropout(p=0.2, inplace=False)
      (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))
    )
    (6): ResnetBlock(
      (norm1): GroupNorm(16, 64, eps=1e-06, affine=True)
      (conv1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))
      (norm2): GroupNorm(16, 64, eps=1e-06, affine=True)
      (dropout): Dropout(p=0.2, inplace=False)
      (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))
    )
    (7): GroupNorm(16, 64, eps=1e-06, affine=True)
    (8): Swish()
    (9): Conv1d(64, 128, kernel_size=(3,), stride=(1,), padding=(1,))
  )
  (final): Linear(in_features=128000, out_features=36, bias=True)
  (dnaemb): Embedding(5, 768)
)
x: torch.Size([256, 1, 1000])
y: torch.Size([256, 36])

------- Start Training --------
Traceback (most recent call last):
  File "/home/wenduoc/mambaforge/envs/geneorca/lib/python3.8/multiprocessing/queues.py", line 245, in _feed
    send_bytes(obj)
  File "/home/wenduoc/mambaforge/envs/geneorca/lib/python3.8/multiprocessing/connection.py", line 200, in send_bytes
    self._send_bytes(m[offset:offset + size])
  File "/home/wenduoc/mambaforge/envs/geneorca/lib/python3.8/multiprocessing/connection.py", line 411, in _send_bytes
    self._send(header + buf)
  File "/home/wenduoc/mambaforge/envs/geneorca/lib/python3.8/multiprocessing/connection.py", line 368, in _send
    n = write(self._handle, buf)
BrokenPipeError: [Errno 32] Broken pipe
[train full 0 0.000010 ] time elapsed: 30.2119 	train loss: 15.1646 	val loss: 14.7883 	val score: 0.4991 	best val score: 0.4991
/home/wenduoc/mambaforge/envs/geneorca/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[train full 1 0.000010 ] time elapsed: 29.8679 	train loss: 15.1287 	val loss: 14.7773 	val score: 0.4976 	best val score: 0.4976
/home/wenduoc/mambaforge/envs/geneorca/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[train full 2 0.000010 ] time elapsed: 28.6659 	train loss: 15.1431 	val loss: 14.7742 	val score: 0.4954 	best val score: 0.4954
/home/wenduoc/mambaforge/envs/geneorca/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[train full 3 0.000010 ] time elapsed: 29.9190 	train loss: 15.2380 	val loss: 14.7722 	val score: 0.4922 	best val score: 0.4922
/home/wenduoc/mambaforge/envs/geneorca/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[train full 4 0.000010 ] time elapsed: 28.7982 	train loss: 15.1334 	val loss: 14.7699 	val score: 0.4879 	best val score: 0.4879
/home/wenduoc/mambaforge/envs/geneorca/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[train full 5 0.000010 ] time elapsed: 29.5297 	train loss: 15.2053 	val loss: 14.7662 	val score: 0.4822 	best val score: 0.4822
/home/wenduoc/mambaforge/envs/geneorca/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[train full 6 0.000010 ] time elapsed: 30.6744 	train loss: 15.1141 	val loss: 14.7614 	val score: 0.4755 	best val score: 0.4755
/home/wenduoc/mambaforge/envs/geneorca/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
[train full 7 0.000010 ] time elapsed: 29.4616 	train loss: 15.1063 	val loss: 14.7551 	val score: 0.4684 	best val score: 0.4684
/home/wenduoc/mambaforge/envs/geneorca/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Traceback (most recent call last):
  File "pretrain_embedder.py", line 489, in <module>
    train_loss = train_one_epoch(model, optimizer, scheduler, train_loader, loss, n_train)
  File "pretrain_embedder.py", line 459, in train_one_epoch
Traceback (most recent call last):
  File "/home/wenduoc/mambaforge/envs/geneorca/lib/python3.8/multiprocessing/queues.py", line 245, in _feed
    send_bytes(obj)
  File "/home/wenduoc/mambaforge/envs/geneorca/lib/python3.8/multiprocessing/connection.py", line 200, in send_bytes
    self._send_bytes(m[offset:offset + size])
  File "/home/wenduoc/mambaforge/envs/geneorca/lib/python3.8/multiprocessing/connection.py", line 411, in _send_bytes
    self._send(header + buf)
  File "/home/wenduoc/mambaforge/envs/geneorca/lib/python3.8/multiprocessing/connection.py", line 368, in _send
    n = write(self._handle, buf)
BrokenPipeError: [Errno 32] Broken pipe
    l.backward()
  File "/home/wenduoc/mambaforge/envs/geneorca/lib/python3.8/site-packages/torch/_tensor.py", line 492, in backward
    torch.autograd.backward(
  File "/home/wenduoc/mambaforge/envs/geneorca/lib/python3.8/site-packages/torch/autograd/__init__.py", line 251, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt
